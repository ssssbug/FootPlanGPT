import os
import time
from typing import Optional, List

from dotenv import load_dotenv
from numpy.matlib import empty
from openai import OpenAI


load_dotenv()

class LLM:
    def __init__(self,
                 model,
                 api_key:Optional[str]=None,
                 base_url:Optional[str]=None,
                 provider:Optional[str]=None,
                 **kwargs):
        if provider.lower() == "chatanywhere":
            try:
                print(f"æ­£åœ¨è°ƒç”¨å…è´¹çš„{model}å¤§æ¨¡å‹é€šè¿‡chatanywhere")
                self.model = model
                self.api_key = os.getenv("CHATANYWHERE_API")
                self.base_url = os.getenv("CHATANYWHERE_BASE_URL")
                self.temperature = kwargs.get("temperature", 0.7)
                self.max_token = kwargs.get("max_token")
                self.timeout = kwargs.get("timeout", 60)
                self.client = OpenAI(api_key=self.api_key,base_url=self.base_url,timeout=60)
            except Exception as e:
                return f"è°ƒç”¨æ¨¡å‹å‡ºé”™{e}"
        #æ ¹æ®æ¨¡å‹åç§°é€‰æ‹©å…·ä½“æ”¯æŒæ¨¡å‹
        else:
            print(f"æ‚¨æ­£åœ¨ä½¿ç”¨è‡ªå·±çš„apikeyè°ƒç”¨{model}æ¨¡å‹ï¼Œè¯·æ³¨æ„tokenæ¶ˆè€—ï¼")
            if model:
                if "gemini" in model.lower():
                    self.model = os.getenv("GEMINI_LLM")
                    self.api_key = os.getenv("GEMINI_API_KEY")
                    if self.model is None or self.api_key is None:
                        raise ValueError(f"è¯·æ£€æŸ¥ä½ çš„{model} model_nameæˆ–è€…{model} API_KEYæ˜¯å¦åœ¨ç¯å¢ƒä¸­æ­£ç¡®è®¾ç½®")
                    print(f"æ‚¨æ­£åœ¨ä½¿ç”¨ ğŸš€ {self.model} æ¨¡å‹")
                elif "claude" in model.lower():
                    self.model = os.getenv("CLAUDE_LLM")
                    self.api_key = os.getenv("CLAUDE_API_KEY")
                    if self.model is None or self.api_key is None:
                        raise ValueError(f"è¯·æ£€æŸ¥ä½ çš„{model} model_nameæˆ–è€…{model} API_KEYæ˜¯å¦åœ¨ç¯å¢ƒä¸­æ­£ç¡®è®¾ç½®")
                    print(f"æ‚¨æ­£åœ¨ä½¿ç”¨ ğŸš€ {self.model} æ¨¡å‹")
                elif "qwen" in model.lower():
                    self.model = os.getenv("QWEN_LLM")
                    self.api_key = os.getenv("QWEN_API_KEY")
                    if self.model is None or self.api_key is None:
                        raise ValueError(f"è¯·æ£€æŸ¥ä½ çš„{model} model_nameæˆ–è€…{model} API_KEYæ˜¯å¦åœ¨ç¯å¢ƒä¸­æ­£ç¡®è®¾ç½®")
                    print(f"æ‚¨æ­£åœ¨ä½¿ç”¨ ğŸš€ {self.model} æ¨¡å‹")
                else:
                    raise ValueError("æ‚¨æ‰€è¾“å…¥çš„æ¨¡å‹æš‚ä¸æ”¯æŒ")
            else:
                raise ValueError("è¯·æ‚¨æ­£ç¡®é…ç½®ä½ çš„model_name,å¯é€‰æ¨¡å‹:[Gemini,Claude,Qwen]")


            # self.model = model or os.getenv("LLM_MODEL_NAME") or "gemini-2.5-flash"
            self.temperature = kwargs.get("temperature",0.7)
            self.max_token  =kwargs.get("max_token")
            self.timeout = kwargs.get("timeout",60)
            #OpenAIçš„baseurl
            self.base_url = base_url or os.getenv("BASE_URL")
            #ä½¿ç”¨è·å–çš„å‚æ•°åˆ›å»ºOpenAIå®¢æˆ·ç«¯å®ä¾‹
            self.client = OpenAI(api_key=self.api_key, base_url=self.base_url,timeout=self.timeout)

    def invoke(self,messages:list[dict[str,str]],**kwargs):
        """
        éæµå¤±è°ƒç”¨LLMï¼Œç›´æ¥è¿”å›å®Œæ•´å“åº”
        :param messages:
        :param kwargs:
        :return:
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_token,
                **{k:v for k,v in kwargs.items() if k not in ['temperature','max_token']}
            )
            # print(response.choices[0].message.content)
            return response.choices[0].message.content
        except Exception as e:
            raise e

    def stream_invoke(self,messages:list[dict[str,str]],**kwargs):
        """
        æµå¤±è°ƒç”¨LLM,å®ç°æ‰“å­—æœºæ•ˆæœ
        :param messages:
        :param kwargs:
        :return:
        """
        try:
            """ä¸¤ç§ä¸åŒå®ç°æ–¹å¼"""
            with self.client.chat.completions.stream(
                    model=self.model,
                    messages=messages,
                    temperature=self.temperature,
                    max_tokens=self.max_token,
            ) as stream:
                for event in stream:
                    if event.type == "content.delta":
                        print(event.delta, end="", flush=True)


            # response = self.client.chat.completions.create(
            #     model=self.model,
            #     messages=messages,
            #     temperature=self.temperature,
            #     max_tokens=self.max_token,
            #     stream=True,
            #     **{k: v for k, v in kwargs.items() if k not in ['temperature', 'max_token']}
            # )
            # for chunk in response:
            #     if hasattr(chunk, "choices") and chunk.choices:
            #         delta = chunk.choices[0].delta
            #         if delta:
            #             for dict_content in delta:
            #                 content = dict_content[1]
            #                 if content is None:
            #                     continue
            #                 for char in content:
            #                     print(char, end="", flush=True)
                                # time.sleep(0.5)
                # else:
                #     break
        except Exception as e:
            raise e




#æµ‹è¯•
if __name__ == "__main__":
    my_llm = LLM(model="gpt-5-mini",provider="chatanywhere")
    message=[{"role":"user","content":"è¯·ä½ ä»‹ç»ä¸€ä¸‹è‡ªå·±"}]
    my_llm.stream_invoke(messages=message)


